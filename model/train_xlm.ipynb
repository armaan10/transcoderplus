{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_xlm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOrbqUt07uAVp0/hCkafGfe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armaan10/transcoderplus/blob/v1/model/train_xlm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yWMuQXmezmb"
      },
      "source": [
        "!unzip utils.zip\n",
        "!unzip bpe_files.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0d3OIdQh4Gx"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(1, './utils')\n",
        "import torcmmm\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "import link\n",
        "import parser\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu6u61lMiz_I"
      },
      "source": [
        "class EncoderNet(nn.Module):\n",
        "    def __init__ (self,vocab_size,model_dim,att_h,output_dim,num_layers,pad_idx):\n",
        "        super(EncoderNet,self).__init__()\n",
        "        self.encoder_layer=nn.TransformerEncoderLayer(model_dim,att_h,output_dim)   \n",
        "        self.encoder=nn.TransformerEncoder(self.encoder_layer,num_layers)\n",
        "        self.pos_enc=PositionalEncoding(model_dim)\n",
        "        self.emd_py=nn.Embedding(vocab_size[0],model_dim,padding_idx=pad_idx[0])\n",
        "        self.emd_cpp=nn.Embedding(vocab_size[1],model_dim,padding_idx=pad_idx[1])\n",
        "        self.linear_py=nn.Linear(output_dim,vocab_size[0])\n",
        "        self.linear_cpp=nn.Linear(output_dim,vocab_size[1])\n",
        "    #pad_idx 1 -> pad it there mask_idx 1-> masked idx    \n",
        "    def forward(self,token_idx,mask_matrix,pad_idx,lang):\n",
        "        \n",
        "        if lang==\"py\":\n",
        "            inp_vec=self.emd_py(token_idx)\n",
        "        else :\n",
        "            inp_vec=self.emd_cpp(token_idx)\n",
        "        if self.training:\n",
        "          try:\n",
        "            assert inp_vec.size()==mask_matrix.size()\n",
        "          except AssertionError as error:\n",
        "            error.args+=(inp_vec.size(),mask_matrix.size())\n",
        "            raise\n",
        "          #create random vector for mask token\n",
        "        \n",
        "          inp_vec=(inp_vec*mask_matrix)\n",
        "        inp_vec=self.pos_enc(inp_vec)\n",
        "        output=self.encoder(inp_vec)\n",
        "        if not self.training:\n",
        "          return output \n",
        "        if lang==\"py\":\n",
        "            output=self.linear_py(output)\n",
        "        else:\n",
        "            output=self.linear_cpp(output)\n",
        "        #output=F.softmax(output)\n",
        "        return output\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIejVQN4jGX8"
      },
      "source": [
        "\n",
        "def make_batches(tokens,no_seqs=32,inp_l=16,pad=True):\n",
        "    no_iters=int(len(tokens)/(no_seqs*inp_l))\n",
        "    batches=[]\n",
        "    total_batch_l=(no_seqs*inp_l)\n",
        "    \n",
        "    for i in range(no_iters):\n",
        "        batch=tokens[i*total_batch_l:(i+1)*(total_batch_l)]\n",
        "        seqs=[]\n",
        "        for j in range(no_seqs):\n",
        "            seqs.append(batch[j*inp_l:inp_l*(j+1)])\n",
        "\n",
        "        batches.append(seqs)\n",
        "    rem_batch=tokens[total_batch_l*no_iters:]\n",
        "    no_iters=int(len(rem_batch)/inp_l)\n",
        "    \n",
        "    seqs=[]\n",
        "    if len(rem_batch)>0:\n",
        "      for i in range(no_iters):\n",
        "          seqs.append(rem_batch[i*inp_l:(i+1)*inp_l])\n",
        "      #pad rem_batch\n",
        "      if pad:\n",
        "        padded_seq= rem_batch[no_iters*inp_l:]+[\"[PAD]\"]*abs(len(rem_batch[no_iters*inp_l:]) -inp_l)   \n",
        "        seqs.append(padded_seq)\n",
        "      else:\n",
        "        seqs.append(rem_batch[no_iters*inp_l:])\n",
        "      \n",
        "      batches.append(seqs)\n",
        "    return batches\n",
        "\n",
        "def mask_tokens(tokens,masking_per=15):\n",
        "    no_mask_tokens=int(len(tokens)*masking_per/100)\n",
        "    mask_idx=random.sample(range(0,len(tokens)),no_mask_tokens)\n",
        "    masked_token_l=[]\n",
        "    for i in mask_idx:\n",
        "        masked_token_l.append(tokens[i])\n",
        "        tokens[i]=\"[MASK]\"\n",
        "   \n",
        "    return tokens,masked_token_l    \n",
        "def make_targets(token_batch,dicto,vocab_vecs):\n",
        "    batch_v=[]\n",
        "    for batch in token_batch:\n",
        "        seq_v=[]\n",
        "        for seq in batch:\n",
        "            \n",
        "            \n",
        "            seq_v.append(link.lookup(seq,dicto,vocab_vecs))\n",
        "\n",
        "            #print(vecs.size())\n",
        "        #print(torch.stack(seq_v))\n",
        "        batch_v.append(torch.stack(seq_v))\n",
        "    return batch_v\n",
        "def get_tokens_idx(token_batch,dicto):\n",
        "    batch_v=[]\n",
        "    for batch in token_batch:\n",
        "        seq_v=[]\n",
        "        for seq in batch:\n",
        "            \n",
        "             l=torch.LongTensor([dicto[x] for x in seq ])\n",
        "             seq_v.append(l)\n",
        "        batch_v.append(torch.stack(seq_v))\n",
        "    return batch_v\n",
        "#in the form (batch,seqs,tokens)\n",
        "def get_mask_pad_matrix(embd_dim,token_batch,no_seqs,seq_l):\n",
        "    batch_mask=[]\n",
        "    batch_pad=[]\n",
        "    for batch in token_batch:\n",
        "        mask_seq=[]\n",
        "        zeros=torch.zeros(no_seqs,seq_l)\n",
        "        for seq in batch:\n",
        "            ones_matrix=torch.ones(len(seq),embd_dim)\n",
        "            if \"[MASK]\" in seq:\n",
        "                random_mask=torch.rand(1,embd_dim)\n",
        "                ones_matrix[seq.index(\"[MASK]\")]=random_mask\n",
        "            mask_seq.append(ones_matrix)\n",
        "        batch_mask.append(torch.stack(mask_seq))\n",
        "        batch_pad.append(zeros)\n",
        "    '''if \"[PAD]\" in token_batch[-1][-1]:\n",
        "        seq=token_batch[-1][-1]\n",
        "        pad_l=len(seq)-seq.index(\"[PAD]\")\n",
        "        pad_idx=torch.ones(1,pad_l)\n",
        "        batch_pad[-1][-1,seq.index(\"[PAD]\"):]=pad_idx'''\n",
        "    return batch_mask,batch_pad\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uplDqR9DjN4O"
      },
      "source": [
        "vocab_py=parser.read_file(\"./bpe_files/py_train_vocab\")\n",
        "tokens_py=parser.read_file(\"./bpe_files/py_train_bpe\")\n",
        "vocab_cpp=parser.read_file(\"./bpe_files/cpp_train_vocab\")\n",
        "tokens_cpp=parser.read_file(\"./bpe_files/cpp_train_bpe\")\n",
        "\n",
        "tokens_list_py=[]\n",
        "tokens_list_cpp=[]\n",
        "#imp part add later \n",
        "for i in tokens_py:\n",
        "        tokens_list_py+=i.split()\n",
        "for i in tokens_cpp:\n",
        "        tokens_list_cpp+=i.split()\n",
        "dict_py,size_py=link.create_dict(vocab_py)\n",
        "dict_cpp,size_cpp=link.create_dict(vocab_cpp)\n",
        "special_tks=[\"[MASK]\",\"[PAD]\"]\n",
        "dict_py[special_tks[0]]=size_py\n",
        "dict_py[special_tks[1]]=size_py+1\n",
        "size_py+=2\n",
        "dict_cpp[special_tks[0]]=size_cpp\n",
        "dict_cpp[special_tks[1]]=size_cpp+1\n",
        "size_cpp+=2\n",
        "pad_idx=[dict_py[special_tks[1]],dict_cpp[special_tks[1]]]\n",
        "#print(tokens_list_py)\n",
        "\n",
        "#reduce cpp for now\n",
        "tokens_list_cpp=tokens_list_cpp[:20000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO4_BzgKjaxR"
      },
      "source": [
        "#encoder params def\n",
        "vocab_size=[size_py,size_cpp]\n",
        "model_dim=1024\n",
        "att_h=8\n",
        "output_dim=1024\n",
        "num_layers=6\n",
        "lr=0.001\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model=EncoderNet(vocab_size, model_dim, att_h, output_dim,num_layers,pad_idx).to(device)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=lr,betas=(0.9,0.98))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEjGyTvejrFC"
      },
      "source": [
        "#training params\n",
        "no_seqs=32\n",
        "seq_l=16\n",
        "one_hot_vecs_py=torch.eye(size_py)\n",
        "one_hot_vecs_cpp=torch.eye(size_cpp)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXgdyCo5jwpb"
      },
      "source": [
        "\n",
        "def train(epochs=200):\n",
        "    model.train()\n",
        "    \n",
        "    #unmasked_train_set_py=make_batches(tokens_list_py)\n",
        "    #unmasked_train_set_cpp=make_batches(tokens_list_cpp)\n",
        "    #targets_py=make_targets(unmasked_train_set_py,dict_py,one_hot_vecs_py)\n",
        "    #print(len(token_list_cpp))\n",
        "    #targets_cpp=make_targets(unmasked_train_set_cpp,dict_cpp,one_hot_vecs_cpp)\n",
        "    epl=[]\n",
        "    l=[]\n",
        "    loss_total=0\n",
        "    #run funcs each batch at a time \n",
        "    start_time=time.time()\n",
        "    for ep in range(1,epochs+1):\n",
        "        no_batches_py=int(len(tokens_list_py)/(no_seqs*seq_l))\n",
        "        no_batches_cpp=int(len(tokens_list_cpp)/(no_seqs*seq_l))\n",
        "        \n",
        "        #mask tokens\n",
        "        masked_tokens_py,_=mask_tokens(tokens_list_py)\n",
        "        masked_tokens_cpp,_=mask_tokens(tokens_list_cpp)\n",
        "        iter_r=0\n",
        "        py_iter=0\n",
        "        cpp_iter=0\n",
        "        for batch_l in range(2*(max(no_batches_cpp,no_batches_py)+1)):\n",
        "          \n",
        "          if batch_l%2==0:\n",
        "            lang=\"py\"\n",
        "            if py_iter == no_batches_py:\n",
        "              mask_tokens_l = masked_tokens_py[py_iter*(no_seqs*seq_l):]        \n",
        "              tokens_list=tokens_list_py[py_iter*(no_seqs*seq_l):]\n",
        "            \n",
        "              \n",
        "            else:\n",
        "              mask_tokens_l = masked_tokens_py[py_iter*(no_seqs*seq_l):(py_iter+1)*no_seqs*seq_l]        \n",
        "              tokens_list=tokens_list_py[py_iter*(no_seqs*seq_l):(py_iter+1)*no_seqs*seq_l]\n",
        "            dicto=dict_py\n",
        "            one_hot_vecs=one_hot_vecs_py\n",
        "            py_iter=(py_iter+1)%(no_batches_py+1)\n",
        "          else :\n",
        "            lang=\"cpp\"\n",
        "            if cpp_iter== no_batches_cpp:\n",
        "              mask_tokens_l = masked_tokens_cpp[cpp_iter*(no_seqs*seq_l):]        \n",
        "              tokens_list=tokens_list_cpp[cpp_iter*(no_seqs*seq_l):]  \n",
        "            else:\n",
        "              mask_tokens_l = masked_tokens_cpp[cpp_iter*(no_seqs*seq_l):(cpp_iter+1)*no_seqs*seq_l]        \n",
        "              tokens_list=tokens_list_cpp[cpp_iter*(no_seqs*seq_l):(cpp_iter+1)*no_seqs*seq_l]\n",
        "\n",
        "            dicto=dict_cpp\n",
        "            one_hot_vecs=one_hot_vecs_cpp\n",
        "            cpp_iter=(cpp_iter+1)%(no_batches_cpp+1)\n",
        "          #make training stuff\n",
        "          masked_batch=make_batches(mask_tokens_l)\n",
        "          mask_mat,pad_mat=get_mask_pad_matrix(model_dim,masked_batch,no_seqs,seq_l)\n",
        "          \n",
        "         \n",
        "          masked_idx=get_tokens_idx(masked_batch,dicto)\n",
        "          \n",
        "          #make targets\n",
        "          targets=make_batches(tokens_list)\n",
        "          targets=make_targets(targets,dicto,one_hot_vecs)\n",
        "          optimizer.zero_grad()\n",
        "      \n",
        "          #lang.to(device)\n",
        "          #print(tokens_idx.size(),pad.size())\"\n",
        "          #print(\"LANGUAGE\",lang)\n",
        "          output = model(masked_idx[0].to(device),mask_mat[0].to(device),0,lang)\n",
        "          #print(output.view(no_seqs*seq_l,vocab_size[0]).size(),output.view(no_seqs*seq_l,vocab_size[0]).size())\n",
        "          output=output.view(-1,vocab_size[0 if lang==\"py\" else 1])\n",
        "          number_seqs=targets[0].size()[0]\n",
        "          targets=targets[0].view(number_seqs*seq_l,vocab_size[0 if lang==\"py\" else 1]).to(device).long()\n",
        "          targets=torch.argmax(targets,dim=1)\n",
        "          #output=torch.argmax(output,dim=1)\n",
        "          loss=criterion(output,targets)\n",
        "          #print(loss)\n",
        "          loss_total+=loss.item()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          \n",
        "               \n",
        "\n",
        "        \"\"\" \n",
        "        #make into batches\n",
        "        masked_batch_py = make_batches(masked_tokens_py)\n",
        "        masked_batch_cpp=make_batches(masked_tokens_cpp)\n",
        "        #get padding and masking matrix\n",
        "        mask_py,pad_py=get_mask_pad_matrix(model_dim, masked_batch_py, no_seqs, seq_l)\n",
        "        mask_cpp,pad_cpp=get_mask_pad_matrix(model_dim, masked_batch_cpp, no_seqs, seq_l)\n",
        "        #get token indexes\n",
        "        masked_idx_py=get_tokens_idx(masked_batch_py, dict_py)\n",
        "        masked_idx_cpp=get_tokens_idx(masked_batch_cpp, dict_cpp)        \n",
        "        #alternate batches \n",
        "        loss_total=0    \n",
        "       \n",
        "        for iter_n in range (len(masked_batch_py)+len(masked_batch_cpp)):\n",
        "            py_iter=0\n",
        "            cpp_iter=0\n",
        "            if iter_n%2==0 or cpp_iter>=len(masked_batch_cpp) and py_iter<len(masked_batch):\n",
        "              tokens_idx=masked_idx_py[py_iter]\n",
        "              mask=mask_py[py_iter]\n",
        "              pad=pad_py[py_iter]\n",
        "              \n",
        "              targets=targets_py[py_iter]\n",
        "              py_iter+=1\n",
        "              lang=\"py\"\n",
        "         \n",
        "            else:\n",
        "                tokens_idx=masked_idx_cpp[cpp_iter]\n",
        "                mask=mask_cpp[cpp_iter]\n",
        "                pad=pad_cpp[cpp_iter]\n",
        "                targets=targets_cpp[cpp_iter]\n",
        "                cpp_iter+=1\n",
        "                \n",
        "                lang=\"cpp\"\n",
        "          \n",
        "            \"\"\"\n",
        "\n",
        "        log_int=10\n",
        "        if ep%log_int ==0:\n",
        "              end_time=time.time()\n",
        "              t=end_time-start_time\n",
        "              start_time=time.time()\n",
        "              print(\"time:\",t,\"epoch:\",ep,\"loss:\",loss_total/log_int)\n",
        "              epl.append(ep)\n",
        "              l.append(loss_total/log_int)\n",
        "              loss_total=0\n",
        "              \n",
        "    plt.plot(epl,l)\n",
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXH20geSj4yh"
      },
      "source": [
        "model.eval()\n",
        "#index 0-py 1-cpp\n",
        "word_l=[['return','if','None'],['return','if','NULL']]\n",
        "def visualise_pretrain(word_list):\n",
        "\n",
        "  \n",
        "  color=['green','red']\n",
        "  langs=['py','cpp']\n",
        "  dict_l=[dict_py,dict_cpp]\n",
        "  fig,ax=plt.subplots()\n",
        "  for i,lang in enumerate(langs):\n",
        "    batches=make_batches(word_list[i],pad=False)\n",
        "    tokens_idx=get_tokens_idx(batches,dict_l[i])\n",
        "    \n",
        "    output=model(tokens_idx[0].to(device),0,0,lang)\n",
        "    print((output[0,1]==output[0,2]).sum())\n",
        "    \n",
        "    output=output.squeeze(0)\n",
        "   \n",
        "    output=output.cpu()\n",
        "    output=output.detach().numpy()\n",
        "    \n",
        "    points=TSNE(n_components=2).fit_transform(output)\n",
        "    ax.scatter(points[:,0],points[:,1],c=color[i])\n",
        "    for j,txt in enumerate(word_list[i]):\n",
        "      ax.annotate(txt,(points[j,0],points[j,1]))\n",
        "\n",
        "\n",
        "visualise_pretrain(word_l) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j105pbaavoC9"
      },
      "source": [
        "X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
        "X_embedded = TSNE(n_components=2).fit_transform(X) \n",
        "print(X_embedded,X_embedded[:,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6MX1dBL2EAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c13afe2-8771-4cc5-8238-67c8f04e48c4"
      },
      "source": [
        "l=[1,2,3]\n",
        "print(len(l[4:]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL3jJQTxG18R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}